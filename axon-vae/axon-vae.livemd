# Axon-VAE

```elixir
Mix.install([
  {:exla, "~> 0.3.0"},
  {:nx, "~> 0.3.0"},
  {:axon, "~> 0.2.0"},
  {:req, "~> 0.3.1"},
  {:kino, "~> 0.6.2"}
])
```

## Intro

The goal of this Notebook is to build a Variational Autoencoder from scratch using Elixir's Axon machine learning library and Livebook. This is my first time using Axon, Livebook and building a VAE, so you'll be following along as I experiment to learn the framework, workflows and VAEs.

## Data loading

A variational autoencoder learns to generate images similar to images it has seen in its dataset. For my first attempt, I'm going to try something simple: generating images of digits using the MNIST digit recognition dataset.

<!-- livebook:{"break_markdown":true} -->

To load the MNIST dataset, I'll use the Axon MNIST tutorial [here](https://github.com/elixir-nx/axon/blob/main/notebooks/mnist.livemd). The steps below are copied over from that tutorial

```elixir
mnist_base_url = "https://storage.googleapis.com/cvdf-datasets/mnist/"
%{body: train_images} = Req.get!(mnist_base_url <> "train-images-idx3-ubyte.gz")
# I'm not going to use the labels so we won't grab them
# %{body: train_labels} = Req.get!(base_url <> "train-labels-idx1-ubyte.gz")

<<_::32, n_images::32, n_rows::32, n_cols::32, images::binary>> = train_images
# Ignore the labels
# <<_::32, n_labels::32, labels::binary>> = train_labels
```

According to [this](http://yann.lecun.com/exdb/mnist/):

Pixels are organized row-wise. Pixel values are 0 to 255. 0 means background (white), 255 means foreground (black).

```elixir
# Inspect the data to see what I'm working with
IO.inspect({n_images, n_rows, n_cols})
# If the pixels are each 1 byte, then these two should match
IO.inspect({n_images * 28 * 28, byte_size(images)})
```

```elixir
images =
  images
  |> Nx.from_binary({:u, 8})
  # Since pixels are organized row-wise, reshape into rows x columns
  |> Nx.reshape({n_images, 1, n_rows, n_cols}, names: [:images, :channels, :height, :width])
  # Normalize the pixel values to be between 0 and 1
  |> Nx.divide(255)
```

```elixir
# Make sure they look like numbers
images[[images: 0..2]] |> Nx.to_heatmap()
```

```elixir
# Download and prepare the test set
mnist_base_url = "https://storage.googleapis.com/cvdf-datasets/mnist/"
%{body: test_images} = Req.get!(mnist_base_url <> "t10k-images-idx3-ubyte.gz")
# I'm not going to use the labels so we won't grab them
# %{body: train_labels} = Req.get!(base_url <> "train-labels-idx1-ubyte.gz")

<<_::32, n_images::32, n_rows::32, n_cols::32, test_images::binary>> = test_images

test_images =
  test_images
  |> Nx.from_binary({:u, 8})
  # Since pixels are organized row-wise, reshape into rows x columns
  |> Nx.reshape({n_images, 1, n_rows, n_cols}, names: [:images, :channels, :height, :width])
  # Normalize the pixel values to be between 0 and 1
  |> Nx.divide(255)

test_images[[images: 0..2]] |> Nx.to_heatmap()
```

## Simple (non-variational) Autoencoder

Since I haven't written a VAE or autoencoder before, I'm going to start by writing a simple autoencoder. My plan is to augment that and turn it into a variational autoencoder once I get this working.

<!-- livebook:{"break_markdown":true} -->

An autoencoder is a a network that has the same sized input as output, with a "bottleneck" layer in the middle with far fewer parameters than the input. Its goal is to force the output to reconstruct the input. The bottleneck layer forces the network to learn a compressed representation of the space.

The part of the autoencder that takes the input and compresses it into the bottleneck layer is called the *encoder* and the part that takes the compressed representation and reconstructs the input is called the *decoder*. Usually the decoder mirrors the encoder.

MNIST is a pretty easy dataset, so I'm going to try a fairly small autoencoder.

The input image has size 784 (28 rows * 28 cols * 1 pixel). I'll set up the encoder to turn that into 256 features, then 128, 64, and then 10 features for the bottleneck layer. The decoder will do the reverse, take the 10 features and go to 64, 128, 256 and 784. I'll use fully-connected (dense) layers.

<!-- livebook:{"break_markdown":true} -->

### The model

```elixir
model =
  Axon.input("image", shape: {nil, 1, 28, 28})
  # This is now 28*28 = 784
  |> Axon.flatten()
  # The encoder
  |> Axon.dense(256, activation: :relu)
  |> Axon.dense(128, activation: :relu)
  |> Axon.dense(64, activation: :relu)
  # Bottleneck layer
  |> Axon.dense(10, activation: :relu)
  # The decoder
  |> Axon.dense(64, activation: :relu)
  |> Axon.dense(128, activation: :relu)
  |> Axon.dense(256, activation: :relu)
  |> Axon.dense(784, activation: :sigmoid)
  # Turn it back into a 28x28 single channel image
  |> Axon.reshape({1, 28, 28})

# We can use Axon.Display to show us what each of the layers would look like
# assuming we send in a batch of 4 images
Axon.Display.as_table(model, Nx.template({4, 1, 28, 28}, :f32)) |> IO.puts()
```

Checking my understanding, since the layers are all dense layers, the number of parameters should be `input_features * output_features` parameters for the weights + `output_features` parameters for the biases for each layer.

This should match the output from Axon.Display

```elixir
# encoder
encoder_parameters = 784 * 256 + 256 + (256 * 128 + 128) + (128 * 64 + 64) + (64 * 10 + 10)
decoder_parameters = 10 * 64 + 64 + (64 * 128 + 128) + (128 * 256 + 256) + (256 * 784 + 784)
total_parameters = encoder_parameters + decoder_parameters
```

### Training

With the model set up, we can now try to train the model. We'll use MSE loss to compare our reconstruction with the original

<!-- livebook:{"break_markdown":true} -->

I'll create the training input by turning our image list into batches of size 128 and then using the same image as both the input and the target. For validation data, I'll use the test images to see how the autoencoder does at reconstructing the test set to make sure I'm not overfitting

```elixir
batch_size = 128

batched_images =
  images
  |> Nx.to_batched(batch_size)

train_data = Stream.zip(batched_images, batched_images)

batched_test_images =
  test_images
  |> Nx.to_batched(batch_size)

test_data = Stream.zip(batched_test_images, batched_test_images)
```

```elixir
params =
  model
  |> Axon.Loop.trainer(:mean_squared_error, Axon.Optimizers.adamw(0.001))
  |> Axon.Loop.validate(model, test_data)
  |> Axon.Loop.run(train_data, %{}, epochs: 5, compiler: EXLA)
```

Now that I have a model that theoretically has learned *something*, let's see what it's learned by running it on some images from the test set. I'll try to use Kino to allow me to select the image it'll run against. To avoid losing the params that took a while to train, I'll create another branch so I can experiment with the params and stop execution when needed without having to retrain.

<!-- livebook:{"branch_parent_index":2} -->

## Evaluation

*A note on branching:*

This section is a branch because I discovered that stopping execution of a cell will cause the entire execution state to be lost. Since I just spent a bunch of time training the model and don't want to lose that memory state, I created a branch so I can stop execution of subsequent cells without worrying about losing the state in the previous section.

I believe what's happening is stopping execution stops the process that's executing the livebook cells and holds the corresponding memory. Creating a branch creates a new process with its own memory that's copied from the branch point, so stopping the cell within the branch only stops that process and prevents the notebook from losing the state in the prior sections.

```elixir
frame = Kino.Frame.new() |> Kino.render()

form =
  Kino.Control.form(
    [
      test_image_index: Kino.Input.number("Test Image Index", default: 0)
    ],
    submit: "Render"
  )

Kino.render(form)

task =
  Task.start(fn ->
    for %{data: %{test_image_index: image_index}} <- Kino.Control.stream(form) do
      test_image = test_images[[images: image_index]]
      # [0] removes the batch dimension
      reconstructed_image = Axon.predict(model, params, test_image)[0]
      combined_image = Nx.concatenate([test_image, reconstructed_image], axis: :width)
      Kino.Frame.render(frame, Nx.to_heatmap(combined_image))
    end
  end)

:ok
```

That looks pretty good!

I used a `Task` above to prevent the `Kino.Control.stream` from blocking execution of the rest of the notebook. If there's a better way to do this, please let me know!

<!-- livebook:{"branch_parent_index":2} -->

## A better training loop

I'd like to see how the model improves as it trains. In this section (also a branch since I plan to experiment and don't want to lose the execution state) I'm to try to improve the training loop to show progress.

`Axon.loop.handle` looks like it might work, specifically with the `:iteration_completed` event

```elixir
combined_input_output = fn params, image_index ->
  test_image = test_images[[images: image_index]]
  reconstructed_image = Axon.predict(model, params, test_image)[0]
  Nx.concatenate([test_image, reconstructed_image], axis: :width)
end

Nx.to_heatmap(combined_input_output.(params, 0))
```

```elixir
frame = Kino.Frame.new() |> Kino.render()

render_example_handler = fn state ->
  Kino.Frame.append(frame, "Epoch: #{state.epoch}, Iteration: #{state.iteration}")
  params = state.step_state[:model_state]
  image_index = Enum.random(0..Nx.axis_size(test_images, :images))
  Kino.Frame.append(frame, Nx.to_heatmap(combined_input_output.(params, image_index)))
  {:continue, state}
end

params =
  model
  |> Axon.Loop.trainer(:mean_squared_error, Axon.Optimizers.adamw(0.001))
  |> Axon.Loop.handle(:iteration_completed, render_example_handler, every: 450)
  |> Axon.Loop.validate(model, test_data)
  |> Axon.Loop.run(train_data, %{}, epochs: 5, compiler: EXLA)
```

Awesome! I have a working autoencoder that I can see getting better in just 5 epochs.

In the next notebook (*coming soon*), I'm going to try to turn this into a VAE.
